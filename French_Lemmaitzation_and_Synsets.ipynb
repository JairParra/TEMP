{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** French lemmatization and synsets with spacy and nltk *** ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OTTAWA | La ministre canadienne des Affaires étrangères, Chrystia Freeland, qui a reçu une délégation du gouvernement mexicain, a estimé mardi que les droits de douane américains sur l'aluminium et l'acier devaient être «abolis» pour «un vrai libre-échange» en Amérique du Nord.\n"
     ]
    }
   ],
   "source": [
    "### Lemmatization ### \n",
    "\n",
    "# Note the wordnet included in the nltk corpus already includes French\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import wordnet as wn \n",
    "\n",
    "# Example political text taken from \n",
    "# https://www.journaldemontreal.com/2019/05/14/front-commun-canada-mexique-contre-les-droits-de-douane-americains-1\n",
    "\n",
    "text = \"\"\" OTTAWA | La ministre canadienne des Affaires étrangères, Chrystia Freeland, qui a reçu une délégation du gouvernement mexicain, a estimé mardi que les droits de douane américains sur l'aluminium et l'acier devaient être «abolis» pour «un vrai libre-échange» en Amérique du Nord.\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***nltk tokenization*** \n",
      "\n",
      "['OTTAWA', '|', 'La', 'ministre', 'canadienne', 'des', 'Affaires', 'étrangères', ',', 'Chrystia', 'Freeland', ',', 'qui', 'a', 'reçu', 'une', 'délégation', 'du', 'gouvernement', 'mexicain', ',', 'a', 'estimé', 'mardi', 'que', 'les', 'droits', 'de', 'douane', 'américains', 'sur', \"l'aluminium\", 'et', \"l'acier\", 'devaient', 'être', '«', 'abolis', '»', 'pour', '«', 'un', 'vrai', 'libre-échange', '»', 'en', 'Amérique', 'du', 'Nord', '.'] \n",
      "\n",
      "[\" OTTAWA | La ministre canadienne des Affaires étrangères, Chrystia Freeland, qui a reçu une délégation du gouvernement mexicain, a estimé mardi que les droits de douane américains sur l'aluminium et l'acier devaient être «abolis» pour «un vrai libre-échange» en Amérique du Nord.\"] \n",
      "\n",
      "***spacy tokenization*** \n",
      "\n",
      "['OTTAWA', 'La', 'ministre', 'canadienne', 'des', 'Affaires', 'étrangères', 'Chrystia', 'Freeland', 'qui', 'a', 'reçu', 'une', 'délégation', 'du', 'gouvernement', 'mexicain', 'a', 'estimé', 'mardi', 'que', 'les', 'droits', 'de', 'douane', 'américains', 'sur', 'aluminium', 'et', 'acier', 'devaient', 'être', 'abolis', 'pour', 'un', 'vrai', 'en', 'Amérique', 'du', 'Nord'] \n",
      "\n",
      "[ OTTAWA |, La ministre canadienne des Affaires étrangères, Chrystia Freeland, qui a reçu une délégation du gouvernement mexicain, a estimé mardi que les droits de douane américains sur l'aluminium et l'acier devaient être «abolis» pour «un vrai libre-échange» en Amérique du Nord.] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Comments: \\n    spacy also does the job '"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Tokenization ###\n",
    "\n",
    "## nltk ## \n",
    "print(\"***nltk tokenization*** \\n\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "word_tokens = [token for token in word_tokenize(text, language='french')] # tokenize by words\n",
    "sent_tokens = [sent for sent in sent_tokenize(text, language='french')] # tokenize by sentence\n",
    "print(word_tokens, \"\\n\") \n",
    "print(sent_tokens, \"\\n\")\n",
    "\n",
    "\"\"\"Comments: \n",
    "    nltk seems to do the job just right.\"\"\"\n",
    "\n",
    "## spacy ## \n",
    "\n",
    "print(\"***spacy tokenization*** \\n\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "# NOTE: A more comprehensive model is available: fr_core_news_md\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "sp_word_tokens = [token.text for token in doc if token.text.isalpha()]\n",
    "sp_sent_tokens = [sent for sent in doc.sents]\n",
    "print(sp_word_tokens, \"\\n\")\n",
    "print(sp_sent_tokens, \"\\n\")\n",
    "\n",
    "\"\"\" Comments: \n",
    "    spacy also does the job \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization ### \n",
    "\n",
    "## nltk ##\n",
    "print(\"**nltk lemmatization***\\n\")\n",
    "\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stemmer = FrenchStemmer() # instantiate the stemmer \n",
    "\n",
    "alpha_words = [w for w in word_tokens if w.isalpha()] # remove punctuation\n",
    "nltk_fr_stems = [(word, stemmer.stem(word)) for word in alpha_words]\n",
    "\n",
    "print(nltk_fr_stems, \"\\n\") \n",
    "\n",
    "\"\"\" Comments: \n",
    "    - As it can be seen, nltk does a very poor job in lemmatizing French.\n",
    "    - An alternative could be to implement our own stemmer with the \n",
    "        stem.Regexp() function, but it looks quite time consuming, \n",
    "        given all the irregularities of the language. In fact, implementing \n",
    "        a new module would be a better idea.\"\"\"\n",
    "\n",
    "## spacy lemmatization: ##\n",
    "print(\"***spacy lemmatization***\\n\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "doc_lemmas = [(token.text, token.lemma_) for token in doc if token.text.isalpha()]\n",
    "print(doc_lemmas)\n",
    "\n",
    "print(\"Lemma: \", doc_lemmas[0][1])\n",
    "\"\"\" Comments: \n",
    "    - Compared to nltk, spacy does an amazing job in lemmatizing French. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***nltk synsets***\n",
      "\n",
      "word : OTTAWA\n",
      "[Synset('ottawa.n.03'), Synset('ottawa.n.01')]\n",
      "\n",
      "word : La\n",
      "[Synset('statistics.n.01'), Synset('statistics.n.01'), Synset('ladyship.n.01'), Synset('la.n.03'), Synset('la.n.03'), Synset('louisiana.n.01'), Synset('lanthanum.n.01'), Synset('lanthanum.n.01'), Synset('lanthanum.n.01')]\n",
      "\n",
      "word : ministre\n",
      "[Synset('minister.n.04'), Synset('indigo_bunting.n.01'), Synset('minister.v.02'), Synset('minister.v.01'), Synset('curate.n.01'), Synset('minister.n.03'), Synset('minister.n.02')]\n",
      "\n",
      "word : canadienne\n",
      "[]\n",
      "\n",
      "word : des\n",
      "[]\n",
      "\n",
      "word : Affaires\n",
      "[]\n",
      "\n",
      "word : étrangères\n",
      "[]\n",
      "\n",
      "word : Chrystia\n",
      "[]\n",
      "\n",
      "word : Freeland\n",
      "[]\n",
      "\n",
      "word : qui\n",
      "[Synset('world_health_organization.n.01')]\n",
      "\n",
      "\n",
      "***spacy + nltk synsets***\n",
      "\n",
      "word : OTTAWA\n",
      "[Synset('ottawa.n.03'), Synset('ottawa.n.01')]\n",
      "\n",
      "word : La\n",
      "[Synset('lordship.n.02'), Synset('lordship.n.01'), Synset('lupus_erythematosus.n.01')]\n",
      "\n",
      "word : ministre\n",
      "[Synset('minister.n.04'), Synset('indigo_bunting.n.01'), Synset('minister.v.02'), Synset('minister.v.01'), Synset('curate.n.01'), Synset('minister.n.03'), Synset('minister.n.02')]\n",
      "\n",
      "word : canadienne\n",
      "[Synset('canadian.a.01'), Synset('canadian.n.02'), Synset('canadian.n.01'), Synset('french_canadian.n.01')]\n",
      "\n",
      "word : des\n",
      "[Synset('matchless.s.01'), Synset('one.s.05'), Synset('one.s.06'), Synset('one.s.04'), Synset('one.s.01'), Synset('one.s.01'), Synset('two.s.01'), Synset('three.s.01'), Synset('one.s.02'), Synset('one.n.02'), Synset('associate_in_nursing.n.01'), Synset('one.n.01'), Synset('one.n.01'), Synset('three.n.01'), Synset('sunday.n.01')]\n",
      "\n",
      "word : Affaires\n",
      "[Synset('matter.n.01'), Synset('business.n.04'), Synset('things.n.01'), Synset('case.n.12')]\n",
      "\n",
      "word : étrangères\n",
      "[Synset('foreign.a.02'), Synset('alien.s.02'), Synset('foreign.a.01'), Synset('alien.s.01'), Synset('estrange.v.02'), Synset('extraneous.s.03'), Synset('alien.v.01'), Synset('extraterrestrial_being.n.01'), Synset('foreigner.n.01'), Synset('foreigner.n.02'), Synset('stranger.n.01')]\n",
      "\n",
      "word : Chrystia\n",
      "[]\n",
      "\n",
      "word : Freeland\n",
      "[]\n",
      "\n",
      "word : qui\n",
      "[Synset('world_health_organization.n.01')]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Comments: \\n    - Better results but several entries are obviously wrong. \\n    - We could filter things like articles and propositions since finding synsets for these \\n        might not be very useful. '"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Synsets ### \n",
    "\n",
    "## nltk ##\n",
    "\n",
    "print(\"***nltk synsets***\\n\")\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Note I decided to use alpha words and not stemmed given that the nltk stemmed words for \n",
    "# French are so imprecise. The result would be a bunch of empty synsets. \n",
    "\n",
    "# create a dictionary of synsets for each word\n",
    "nltk_synsets_fr = {word: wn.synsets(word, lang='fra') for word in alpha_words}\n",
    "\n",
    "def print_synsets(synset, num_synsets=10):\n",
    "    \"\"\" expects a dictionary with synsets. \n",
    "        prints num_synsets number of synsets in the dictionary\"\"\"\n",
    "    i = 0\n",
    "    for key, val in synset.items():\n",
    "        if i < num_synsets: \n",
    "            print(\"word : {}\".format(key))\n",
    "            print(val)\n",
    "            i += 1\n",
    "            print()\n",
    "    \n",
    "print_synsets(nltk_synsets_fr)\n",
    "\n",
    "\"\"\" Comments: \n",
    "    Many words are missing synsets. \"\"\"\n",
    "\n",
    "## spacy + nltk ##\n",
    "\n",
    "print(\"\\n***spacy + nltk synsets***\\n\")\n",
    "\n",
    "spnlkt_synsest_fr = {tup[0]: wn.synsets(tup[1], lang='fra') for tup in doc_lemmas}\n",
    "\n",
    "print_synsets(spnlkt_synsest_fr) \n",
    "\n",
    "\"\"\" Comments: \n",
    "    - Better results but several entries are obviously wrong. \n",
    "    - We could filter things like articles and propositions since finding synsets for these \n",
    "        might not be very useful. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***improved spacy+nltk***\n",
      "\n",
      "**filtered doc lemmas**\n",
      "\n",
      "[('OTTAWA', 'ottawa'), ('ministre', 'ministre'), ('canadienne', 'canadien'), ('Affaires', 'affaire'), ('étrangères', 'étranger'), ('Chrystia', 'Chrystia'), ('Freeland', 'Freeland'), ('qui', 'qui'), ('a', 'avoir'), ('reçu', 'recevoir'), ('délégation', 'délégation'), ('gouvernement', 'gouvernement'), ('mexicain', 'mexicain'), ('a', 'avoir'), ('estimé', 'estimer'), ('mardi', 'mardi'), ('que', 'que'), ('droits', 'droit'), ('américains', 'américain'), ('aluminium', 'aluminium'), ('acier', 'acier'), ('devaient', 'devoir'), ('être', 'être'), ('abolis', 'aboli'), ('vrai', 'vrai'), ('Amérique', 'Amérique'), ('Nord', 'nord')]\n",
      "\n",
      "**synsets**\n",
      "\n",
      "word : ottawa\n",
      "[Synset('ottawa.n.03'), Synset('ottawa.n.01')]\n",
      "\n",
      "word : ministre\n",
      "[Synset('minister.n.04'), Synset('indigo_bunting.n.01'), Synset('minister.v.02'), Synset('minister.v.01'), Synset('curate.n.01'), Synset('minister.n.03'), Synset('minister.n.02')]\n",
      "\n",
      "word : canadien\n",
      "[Synset('canadian.a.01'), Synset('canadian.n.02'), Synset('canadian.n.01'), Synset('french_canadian.n.01')]\n",
      "\n",
      "word : affaire\n",
      "[Synset('matter.n.01'), Synset('business.n.04'), Synset('things.n.01'), Synset('case.n.12')]\n",
      "\n",
      "word : étranger\n",
      "[Synset('foreign.a.02'), Synset('alien.s.02'), Synset('foreign.a.01'), Synset('alien.s.01'), Synset('estrange.v.02'), Synset('extraneous.s.03'), Synset('alien.v.01'), Synset('extraterrestrial_being.n.01'), Synset('foreigner.n.01'), Synset('foreigner.n.02'), Synset('stranger.n.01')]\n",
      "\n",
      "word : Chrystia\n",
      "[]\n",
      "\n",
      "word : Freeland\n",
      "[]\n",
      "\n",
      "word : qui\n",
      "[Synset('world_health_organization.n.01')]\n",
      "\n",
      "word : avoir\n",
      "[Synset('beget.v.01'), Synset('give_birth.v.01'), Synset('have.v.12'), Synset('suffer.v.02'), Synset('contract.v.04'), Synset('grow.v.08'), Synset('get.v.03'), Synset('have.v.11'), Synset('receive.v.02'), Synset('catch.v.18'), Synset('get.v.20'), Synset('interview.v.01'), Synset('interview.v.03'), Synset('retention.n.01'), Synset('old.s.07'), Synset('deceive.v.02'), Synset('old.s.03'), Synset('drive.v.11'), Synset('get.v.14'), Synset('honest-to-god.s.01'), Synset('sleep_together.v.01'), Synset('take.v.35'), Synset('bring.v.04'), Synset('old.a.02'), Synset('draw.v.15'), Synset('old.a.01'), Synset('hold.v.03'), Synset('catch.v.24'), Synset('get.v.27'), Synset('scram.v.01'), Synset('experience.v.03'), Synset('have.v.01'), Synset('own.v.01'), Synset('have.v.09'), Synset('get.v.22'), Synset('receive.v.01'), Synset('accept.v.02'), Synset('obtain.v.01'), Synset('have.v.17'), Synset('get.v.21'), Synset('have.v.07'), Synset('deceive.v.01'), Synset('flim-flam.v.01'), Synset('have.v.02'), Synset('carry.v.21'), Synset('prevail.v.02'), Synset('embody.v.02'), Synset('carry.v.02'), Synset('have.v.10'), Synset('be.v.12'), Synset('carry.v.18'), Synset('minister.n.03'), Synset('rich_person.n.01'), Synset('property.n.01'), Synset('credit_side.n.01')]\n",
      "\n",
      "word : recevoir\n",
      "[Synset('beget.v.01'), Synset('suffer.v.02'), Synset('grow.v.08'), Synset('receive.v.08'), Synset('get.v.03'), Synset('receive.v.02'), Synset('catch.v.18'), Synset('receive.v.13'), Synset('get.v.20'), Synset('receive.v.06'), Synset('entertain.v.02'), Synset('welcome.v.02'), Synset('get.v.14'), Synset('receive.v.12'), Synset('get.v.19'), Synset('bring.v.04'), Synset('receive.v.05'), Synset('catch.v.07'), Synset('draw.v.15'), Synset('catch.v.24'), Synset('pick_up.v.09'), Synset('get.v.25'), Synset('catch.v.22'), Synset('catch.v.21'), Synset('get.v.22'), Synset('receive.v.01'), Synset('accept.v.09'), Synset('accept.v.02'), Synset('accept.v.05'), Synset('bear.v.06'), Synset('get.v.21'), Synset('receive.v.10'), Synset('meet.v.11')]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Comments: \\n    - Improvement from the previous implementations. \\n    - tags can be easily adjusted if one wishes to include them in the list comprehension. \\n    - NOTE:  '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  improved spacy + nltk ## \n",
    "\n",
    "print(\"***improved spacy+nltk***\\n\")\n",
    "\n",
    "# create a filtered lemmas to exclude determiners (DET), adpositions(aka prepositions) (ADP), \n",
    "# punctuation (PUNCT), conjuctions (CONJ,CCONJ), numerals (NUM), symbols (SYM), spaces (NUM), \n",
    "# and non-alpha tokens. \n",
    "# Full list can be found at https://spacy.io/api/annotation\n",
    "\n",
    "tags = [\"DET\",\"ADP\",\"PUNCT\",\"CONJ\",\"CCONJ\",\"NUM\",\"SYM\",\"SPACE\"]\n",
    "flt_doc_lemmas = [(token.text, token.lemma_) for token in doc if token.pos_ not in tags and token.text.isalpha()]\n",
    "\n",
    "print(\"**filtered doc lemmas**\\n\")\n",
    "print(flt_doc_lemmas)\n",
    "\n",
    "# Now we will generate the synsets using these filtered lemmaS\n",
    "print(\"\\n**synsets**\\n\")\n",
    "\n",
    "# list comprehension of lemmatized words\n",
    "flt_synsets_fr = {tup[1]: wn.synsets(tup[1], lang='fra') for tup in flt_doc_lemmas}\n",
    "\n",
    "print_synsets(flt_synsets_fr)\n",
    "\n",
    "\"\"\"Comments: \n",
    "    - Improvement from the previous implementations. \n",
    "    - tags can be easily adjusted if one wishes to include them in the list comprehension. \n",
    "    - NOTE:  \"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spaCy NER & visualization ### \n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(\"[Entity:{}] [start:{}] [end:{}] [Label:{}]\".format(ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "    \n",
    "displacy.render(doc, style=\"ent\",jupyter=True)\n",
    "\n",
    "# Another example taken from https://www.lemonde.fr/international/article/2019/05/11/au-mexique-la-double-crise-migratoire-tourne-au-casse-tete-pour-amlo_5460889_3210.html\n",
    "text = \"\"\"Chaque jour, les sans-papiers franchissent par centaines, parfois par milliers, le fleuve Suchiate, qui sépare le Guatemala du Mexique. Leur nombre est évalué à plus de 300 000 lors du premier trimestre 2019 par l’Institut national mexicain des migrations (INM), soit trois à quatre fois plus que les années précédentes. Du jamais-vu lié au nouveau phénomène des caravanes de Centraméricains, fuyant ensemble la misère et la violence de leurs pays d’origine. Ce tsunami migratoire provoque des goulots d’étranglement. A Tapachula, principale ville frontalière avec le Guatemala, les clandestins attendent en masse des permis de transit pour continuer leur route vers les Etats-Unis.\"\"\" \n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\",jupyter=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
